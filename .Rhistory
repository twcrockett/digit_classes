def hosvd_transform(X_train, X_test, n_components_modes=(10, 10)):
"""Apply HOSVD to train data and transform test data"""
# Reshape data to 3D tensor
X_train_tensor = reshape_for_hosvd(X_train)
X_test_tensor = reshape_for_hosvd(X_test)
# Get tensor dimensions
n_samples = X_train_tensor.shape[0]  # Mode 0 dimension (samples)
n_features_1 = X_train_tensor.shape[1]  # Mode 1 dimension
n_features_2 = X_train_tensor.shape[2]  # Mode 2 dimension
# Set up consistent ranks for all modes
# (Modes are zero-indexed in the rank parameter)
rank = [n_samples,  # Mode 0 (samples) rank
min(n_features_1, n_components_modes[0]),  # Mode 1 feature rank
min(n_features_2, n_components_modes[1])]  # Mode 2 feature rank
# Perform Tucker decomposition
core, factors = tucker(X_train_tensor, rank=rank)
# Take appropriate subset of factors
factors_subset = factors.copy()
factors_subset[0] = factors_subset[0][:X_test_tensor.shape[0],:X_test_tensor.shape[0]]
X_train_hosvd = tl.tucker_to_tensor((core, factors))
spatial_modes = list(range(1, tl.ndim(X_test_tensor)))
X_test_projected = X_test_tensor.copy()
# Project to factor space for spatial dimensions
for mode in spatial_modes:
X_test_projected = tl.tenalg.mode_dot(X_test_projected, factors[mode].T, mode)
# Project back to original space for spatial dimensions
X_test_hosvd = X_test_projected.copy()
for mode in spatial_modes:
X_test_hosvd = tl.tenalg.mode_dot(X_test_hosvd, factors[mode], mode)
return X_train_hosvd, X_test_hosvd, factors
best_params['HOSVD'] = optimize_hyperparameters(X, y, 'HOSVD', param_grids['HOSVD'], skf)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
n_components_1 = params['n_components_1']
n_components_1 = [5, 10, 15]
n_components_2 = [5, 10, 15]
X_train_hosvd, X_test_hosvd, _ = hosvd_transform(
X_train_scaled, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
X_train_hosvd, X_test_hosvd, _ = hosvd_transform(
X_train_scaled, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
param_grids['HOSVD']
params = param_grids['HOSVD']
n_components_1 = params['n_components_1']
n_components_2 = params['n_components_2']
classifier = params['classifier']
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Apply HOSVD
X_train_hosvd, X_test_hosvd, _ = hosvd_transform(
X_train_scaled, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
n_components_1 = 5
n_components_2 = 5
X_train_hosvd, X_test_hosvd, _ = hosvd_transform(
X_train_scaled, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
# Train classifier
clf = classifier()
classifier
clf = classifier()
clf=classifier
clf.fit(X_train_hosvd, y_train)
clf
clf()
classifier()
classifier().fit()
SVC
SVC.fit
clf = SVC
clf.fit(X_train_hosvd, y_train)
y_train = y[all_indices[:split_idx]]
y_test = y[all_indices[split_idx:]]
clf.fit(X_train_hosvd, y_train)
X_train_hosvd
clf.fit(X_train_hosvd, y_train)
clf = SVC()
clf.fit(X_train_hosvd, y_train)
class ReconstructionClassifier(BaseEstimator, ClassifierMixin):
"""
Classifier based on reconstruction error using Frobenius norm residuals.
"""
def __init__(self):
self.class_models = {}
def fit(self, X, y):
"""
Fit a separate model for each class
Parameters:
-----------
X : array-like, shape (n_samples, n_features)
Training data
y : array-like, shape (n_samples,)
Target values
Returns:
--------
self : object
"""
self.classes_ = np.unique(y)
# Store models for each class
for cls in self.classes_:
# Get data for this class
X_cls = X[y == cls]
# Store the class mean for reconstruction error calculation
self.class_models[cls] = {
'mean': np.mean(X_cls, axis=0),
'data': X_cls  # Store class data for distance computation
}
return self
def predict(self, X):
"""
Predict class for X based on lowest reconstruction error.
Parameters:
-----------
X : array-like, shape (n_samples, n_features)
The input data.
Returns:
--------
y_pred : array, shape (n_samples,)
Class labels for samples in X.
"""
if not hasattr(self, 'classes_'):
raise ValueError("Classifier not fitted yet.")
# Calculate reconstruction error for each class
errors = np.zeros((X.shape[0], len(self.classes_)))
for i, cls in enumerate(self.classes_):
# Calculate Frobenius norm of the difference between each sample and class data
class_data = self.class_models[cls]['data']
# For each test sample, find its reconstruction error to this class
for j, sample in enumerate(X):
# Calculate distances to all samples in this class
distances = np.linalg.norm(class_data - sample, ord='fro', axis=1)
# Use the minimum distance as the reconstruction error
errors[j, i] = np.min(distances)
# Select class with minimum error
return self.classes_[np.argmin(errors, axis=1)]
from sklearn.base import BaseEstimator, ClassifierMixin
class ReconstructionClassifier(BaseEstimator, ClassifierMixin):
"""
Classifier based on reconstruction error using Frobenius norm residuals.
"""
def __init__(self):
self.class_models = {}
def fit(self, X, y):
"""
Fit a separate model for each class
Parameters:
-----------
X : array-like, shape (n_samples, n_features)
Training data
y : array-like, shape (n_samples,)
Target values
Returns:
--------
self : object
"""
self.classes_ = np.unique(y)
# Store models for each class
for cls in self.classes_:
# Get data for this class
X_cls = X[y == cls]
# Store the class mean for reconstruction error calculation
self.class_models[cls] = {
'mean': np.mean(X_cls, axis=0),
'data': X_cls  # Store class data for distance computation
}
return self
def predict(self, X):
"""
Predict class for X based on lowest reconstruction error.
Parameters:
-----------
X : array-like, shape (n_samples, n_features)
The input data.
Returns:
--------
y_pred : array, shape (n_samples,)
Class labels for samples in X.
"""
if not hasattr(self, 'classes_'):
raise ValueError("Classifier not fitted yet.")
# Calculate reconstruction error for each class
errors = np.zeros((X.shape[0], len(self.classes_)))
for i, cls in enumerate(self.classes_):
# Calculate Frobenius norm of the difference between each sample and class data
class_data = self.class_models[cls]['data']
# For each test sample, find its reconstruction error to this class
for j, sample in enumerate(X):
# Calculate distances to all samples in this class
distances = np.linalg.norm(class_data - sample, ord='fro', axis=1)
# Use the minimum distance as the reconstruction error
errors[j, i] = np.min(distances)
# Select class with minimum error
return self.classes_[np.argmin(errors, axis=1)]
X_train_hosvd, X_test_hosvd, _ = hosvd_transform(
X_train_scaled, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
classes = np.unique(y_train)
classes
error_features = []
idx = y_train == 1
X_train_cls = X_train_scaled[idx]
cls_X_train_hosvd, cls_X_test_hosvd, _ = hosvd_transform(
X_train_cls, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
reconstruction_errors = np.linalg.norm(
X_test_scaled - cls_X_test_hosvd,
ord='fro',
axis=1
).reshape(-1, 1)
X_test_hosvd.shape
X_train_scaled = scaler.fit_transform(X_train).reshape_for_hosvd()
X_test_scaled = scaler.transform(X_test).reshape_for_hosvd()
X_test_scaled.shape
X_test_scaled = scaler.transform(X_test).reshape_for_hosvd()
X_test_scaled = reshape_for_hosvd(scaler.transform(X_test))
X_test_scaled.shape
reconstruction_errors = np.linalg.norm(
X_test_scaled - cls_X_test_hosvd,
ord='fro',
axis=1
).reshape(-1, 1)
X_test_scaled.shape
reconstruction_errors = np.array([
np.linalg.norm(
X_test_scaled[i] - cls_X_test_hosvd[i],
ord='fro'
) for i in range(X_test_scaled.shape[0])
]).reshape(-1, 1)
reconstruction_errors
def reshape_for_hosvd(X, img_shape=(16, 16)):
"""Reshape flattened image data to 3D tensor (samples, height, width)"""
n_samples = X.shape[0]
return X.reshape((n_samples, img_shape[0], img_shape[1]))
def hosvd_transform(X_train_tensor, X_test_tensor, n_components_modes=(10, 10)):
"""Apply HOSVD to train data and transform test data"""
# Get tensor dimensions
n_samples = X_train_tensor.shape[0]  # Mode 0 dimension (samples)
n_features_1 = X_train_tensor.shape[1]  # Mode 1 dimension
n_features_2 = X_train_tensor.shape[2]  # Mode 2 dimension
# Set up consistent ranks for all modes
# (Modes are zero-indexed in the rank parameter)
rank = [n_samples,  # Mode 0 (samples) rank
min(n_features_1, n_components_modes[0]),  # Mode 1 feature rank
min(n_features_2, n_components_modes[1])]  # Mode 2 feature rank
# Perform Tucker decomposition
core, factors = tucker(X_train_tensor, rank=rank)
# Take appropriate subset of factors
factors_subset = factors.copy()
factors_subset[0] = factors_subset[0][:X_test_tensor.shape[0],:X_test_tensor.shape[0]]
X_train_hosvd = tl.tucker_to_tensor((core, factors))
spatial_modes = list(range(1, tl.ndim(X_test_tensor)))
X_test_projected = X_test_tensor.copy()
# Project to factor space for spatial dimensions
for mode in spatial_modes:
X_test_projected = tl.tenalg.mode_dot(X_test_projected, factors[mode].T, mode)
# Project back to original space for spatial dimensions
X_test_hosvd = X_test_projected.copy()
for mode in spatial_modes:
X_test_hosvd = tl.tenalg.mode_dot(X_test_hosvd, factors[mode], mode)
return X_train_hosvd.reshape(n_samples, -1), X_test_hosvd.reshape(X_test_tensor.shape[0], -1), factors
def evaluate_fold(X_train, X_test, y_train, y_test, method, params=None):
"""Evaluate a method on a single fold with specific parameters"""
start_time = time.time()
if method == 'SVD':
n_components = params['n_components']
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Apply SVD
svd = TruncatedSVD(n_components=n_components)
X_train_svd = svd.fit_transform(X_train_scaled)
# Create class-specific SVD models for reconstruction-based classification
class_models = {}
classes = np.unique(y_train)
# Calculate reconstructed data for training set
X_train_reconstructed = svd.inverse_transform(X_train_svd)
# Calculate reconstruction error for test set
X_test_svd = svd.transform(X_test_scaled)
X_test_reconstructed = svd.inverse_transform(X_test_svd)
# Create training dataset with reconstruction errors per class
error_features = []
for cls in classes:
# Get indices for this class
idx = y_train == cls
# Create SVD model for this specific class
cls_svd = TruncatedSVD(n_components=n_components)
cls_X_train_svd = cls_svd.fit_transform(X_train_scaled[idx])
# Transform test data with class-specific SVD
cls_X_test_svd = cls_svd.transform(X_test_scaled)
# Reconstruct test data with class-specific SVD
cls_X_test_reconstructed = cls_svd.inverse_transform(cls_X_test_svd)
# Calculate Frobenius norm of reconstruction error
reconstruction_errors = np.linalg.norm(
X_test_scaled - cls_X_test_reconstructed,
ord='fro',
axis=1
).reshape(-1, 1)
error_features.append(reconstruction_errors)
# Combine error features from all classes
X_test_errors = np.hstack(error_features)
# Predict based on minimum reconstruction error
y_pred = classes[np.argmin(X_test_errors, axis=1)]
elif method == 'HOSVD':
n_components_1 = params['n_components_1']
n_components_2 = params['n_components_2']
# Scale data
scaler = StandardScaler()
X_train_scaled = reshape_for_hosvd(scaler.fit_transform(X_train))
X_test_scaled = reshape_for_hosvd(scaler.transform(X_test))
# Apply HOSVD for overall data
X_train_hosvd, X_test_hosvd, _ = hosvd_transform(
X_train_scaled, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
# Create class-specific HOSVD models for reconstruction error calculation
classes = np.unique(y_train)
error_features = []
for cls in classes:
# Get data for this class
idx = y_train == cls
X_train_cls = X_train_scaled[idx]
# Apply class-specific HOSVD
cls_X_train_hosvd, cls_X_test_hosvd, _ = hosvd_transform(
X_train_cls, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
# Calculate Frobenius norm of reconstruction error
reconstruction_errors = np.array([
np.linalg.norm(
X_test_scaled[i] - cls_X_test_hosvd[i],
ord='fro'
) for i in range(X_test_scaled.shape[0])
]).reshape(-1, 1)
error_features.append(reconstruction_errors)
# Combine error features from all classes
X_test_errors = np.hstack(error_features)
# Predict based on minimum reconstruction error
y_pred = classes[np.argmin(X_test_errors, axis=1)]
elif method == 'RandomForest':
n_estimators = params['n_estimators']
max_depth = params['max_depth']
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Train Random Forest
clf = RandomForestClassifier(
n_estimators=n_estimators,
max_depth=max_depth,
random_state=48
)
clf.fit(X_train_scaled, y_train)
# Predict
y_pred = clf.predict(X_test_scaled)
elif method == 'NaiveBayes':
var_smoothing = params['var_smoothing']
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Train Naive Bayes
clf = GaussianNB(var_smoothing=var_smoothing)
clf.fit(X_train_scaled, y_train)
# Predict
y_pred = clf.predict(X_test_scaled)
elif method == 'SVM':
C = params['C']
kernel = params['kernel']
gamma = params['gamma']
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Train SVM
clf = SVC(C=C, kernel=kernel, gamma=gamma, random_state=48)
clf.fit(X_train_scaled, y_train)
# Predict
y_pred = clf.predict(X_test_scaled)
else:
raise ValueError(f"Unknown method: {method}")
# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
# Calculate time taken
elapsed_time = time.time() - start_time
return {
'accuracy': accuracy,
'time': elapsed_time,
'y_pred': y_pred
}
def optimize_hyperparameters(X, y, method, param_grid, skf):
"""Find optimal hyperparameters for a given method using k-fold CV"""
print(f"\nOptimizing hyperparameters for {method}...")
# Store results for each parameter combination
param_results = []
# Try each parameter combination
for params in ParameterGrid(param_grid):
fold_accuracies = []
fold_times = []
# Evaluate on each fold
for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):
X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]
result = evaluate_fold(X_train, X_test, y_train, y_test, method, params)
fold_accuracies.append(result['accuracy'])
fold_times.append(result['time'])
# Calculate average metrics
mean_accuracy = np.mean(fold_accuracies)
mean_time = np.mean(fold_times)
# Store results
param_results.append({
'params': params,
'mean_accuracy': mean_accuracy,
'mean_time': mean_time,
'std_accuracy': np.std(fold_accuracies)
})
param_str = ", ".join(f"{k}={v}" for k, v in params.items())
print(f"  {param_str} - Accuracy: {mean_accuracy:.4f} ± {np.std(fold_accuracies):.4f}, Time: {mean_time:.2f}s")
# Find best parameters
best_result = max(param_results, key=lambda x: x['mean_accuracy'])
print(f"Best parameters for {method}: {best_result['params']}")
print(f"Best accuracy: {best_result['mean_accuracy']:.4f}")
return best_result['params']
def compare_methods(X, y, methods, best_params, n_folds=5):
"""Compare best models from each method using greedy k-fold CV"""
print("\nComparing methods using greedy k-fold CV...")
# Create consistent k-fold splits for method comparison
kf = KFold(n_splits=n_folds, shuffle=True, random_state=48)
fold_indices = [(train_idx, test_idx) for train_idx, test_idx in kf.split(X)]
# Store results for each method
method_results = {method: {'accuracies': [], 'times': [], 'predictions': []} for method in methods}
# Evaluate each method on the same folds
for fold_idx, (train_idx, test_idx) in enumerate(fold_indices):
X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]
print(f"\nFold {fold_idx+1}/{n_folds}")
for method in methods:
params = best_params[method]
result = evaluate_fold(X_train, X_test, y_train, y_test, method, params)
method_results[method]['accuracies'].append(result['accuracy'])
method_results[method]['times'].append(result['time'])
method_results[method]['predictions'].append(result['y_pred'])
print(f"  {method} - Accuracy: {result['accuracy']:.4f}, Time: {result['time']:.2f}s")
# Calculate and display overall results
print("\nOverall Results:")
overall_results = []
for method in methods:
mean_accuracy = np.mean(method_results[method]['accuracies'])
std_accuracy = np.std(method_results[method]['accuracies'])
mean_time = np.mean(method_results[method]['times'])
overall_results.append({
'method': method,
'mean_accuracy': mean_accuracy,
'std_accuracy': std_accuracy,
'mean_time': mean_time
})
print(f"{method} - Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}, Time: {mean_time:.2f}s")
return method_results, overall_results
best_params['SVD'] = optimize_hyperparameters(X, y, 'SVD', param_grids['SVD'], skf)
best_params['HOSVD'] = optimize_hyperparameters(X, y, 'HOSVD', param_grids['HOSVD'], skf)
X_test_errors = np.hstack(error_features)
classes = np.unique(y_train)
error_features = []
for cls in classes:
# Get data for this class
idx = y_train == cls
X_train_cls = X_train_scaled[idx]
# Apply class-specific HOSVD
cls_X_train_hosvd, cls_X_test_hosvd, _ = hosvd_transform(
X_train_cls, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
# Calculate Frobenius norm of reconstruction error
reconstruction_errors = np.array([
for cls in classes:
# Get data for this class
idx = y_train == cls
X_train_cls = X_train_scaled[idx]
# Apply class-specific HOSVD
cls_X_train_hosvd, cls_X_test_hosvd, _ = hosvd_transform(
X_train_cls, X_test_scaled,
n_components_modes=(n_components_1, n_components_2)
)
# Calculate Frobenius norm of reconstruction error
reconstruction_errors = np.array([
np.linalg.norm(
X_test_scaled[i] - cls_X_test_hosvd[i],
ord='fro'
) for i in range(X_test_scaled.shape[0])
]).reshape(-1, 1)
error_features.append(reconstruction_errors)
error_features
error_features.shape
error_features.shape()
error_features.shape
error_features
View(error_features)
X_test_errors = np.hstack(error_features)
X_test_errors
X_test_errors.shape
y_pred = classes[np.argmin(X_test_errors, axis=1)]
y_pred
