# Create a common train/test split for all evaluations
np.random.seed(48)
g_indices = np.random.permutation(len(X))
split_point = int(len(X) * 0.8)
gtrain_indices = g_indices[:split_point]
gtest_indices = g_indices[split_point:]
X_gtrain, X_gtest = X[gtrain_indices], X[gtest_indices]
y_gtrain, y_gtest = y[gtrain_indices], y[gtest_indices]
# Perform greedy cross-validation
greedy_results = []
remaining_models = list(model_functions.keys())
iteration = 1
while remaining_models:
print(f"\nIteration {iteration} - Evaluating {len(remaining_models)} models")
iteration_results = []
for model_name in remaining_models:
model_func = model_functions[model_name]
start_time = time.time()
y_pred = model_func(X_gtrain, X_gtest, y_gtrain, y_gtest)
elapsed_time = time.time() - start_time
accuracy = accuracy_score(y_gtest, y_pred)
report = classification_report(y_gtest, y_pred, output_dict=True)
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
# Calculate selection score
t_m = 60 * 30  # 30 minutes in seconds
selection_score = (1 / (1 + np.exp(27 - 30 * f1))) - (np.exp(27) / (1 + np.exp(27))) + np.exp(-elapsed_time / t_m)
# Store results for this model
result = {
'iteration': iteration,
'model': model_name,
'mean_accuracy': accuracy,
'mean_precision': precision,
'mean_recall': recall,
'f1_score': f1,
'total_time': elapsed_time,
'selection_score': selection_score
}
# Add the best hyperparameters to the result
best_config = best_configs[model_name]
for param, value in best_config.items():
if param not in ['mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']:
result[param] = value
iteration_results.append(result)
print(f"  {model_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}, Time={elapsed_time:.2f}s, Score={selection_score:.4f}")
# Find the best model in this iteration
iteration_df = pd.DataFrame(iteration_results)
best_idx = iteration_df['selection_score'].idxmax()
best_model = iteration_df.loc[best_idx, 'model']
# Add all results from this iteration to the overall results
greedy_results.extend(iteration_results)
# Keep only the best model for the next iteration
remaining_models.remove(best_model)
print(f"  Best model in iteration {iteration}: {best_model} (Score: {iteration_df.loc[best_idx, 'selection_score']:.4f})")
# Stop after one iteration for greedy approach
break
greedy_df = pd.DataFrame(greedy_results)
greedy_df = greedy_df.sort_values('selection_score', ascending=False).reset_index(drop=True)
def format_model_name(model_data, prefix):
if prefix == 'LogisticRegression':
return f"Logistic Regression (C={model_data['C']})"
elif prefix == 'SVD':
return f"SVD-{model_data['classifier']} (k={model_data['n_components']})"
elif prefix == 'HOSVD':
return f"HOSVD-{model_data['classifier']} (r1={model_data['n_components_mode1']}, r2={model_data['n_components_mode2']})"
elif prefix == 'RandomForest':
return f"Random Forest (t={model_data['n_estimators']}, d={model_data['max_depth'] or 'None'})"
elif prefix == 'GradientBoosting':
return f"Gradient Boosting (t={model_data['n_estimators']}, lr={model_data['learning_rate']}, d={model_data['max_depth']})"
elif prefix == 'NaiveBayes':
return f"Naive Bayes (s={model_data['var_smoothing']})"
elif prefix == 'SVM':
return f"SVM (C={model_data['C']}, k={model_data['kernel']}, g={model_data['gamma']})"
elif prefix == 'KNN':
return f"KNN (k={model_data['n_neighbors']}, w={model_data['weights']}, d={model_data['p']})"
elif prefix == 'LDA':
return f"LDA (solver={model_data['solver']}, shrink={model_data['shrinkage'] or 'None'})"
else:
return prefix
# Format model names in the greedy results dataframe
for i, row in greedy_df.iterrows():
greedy_df.at[i, 'model'] = format_model_name(row, row['model'])
greedy_df = greedy_df[['model', 'mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']].copy()
greedy_df = greedy_df.sort_values('selection_score', ascending=False).reset_index(drop=True)
best_configs = {
'LogisticRegression': lr_df.loc[lr_df['selection_score'].idxmax()].to_dict(),
'SVD': svd_df.loc[svd_df['selection_score'].idxmax()].to_dict(),
'HOSVD': hosvd_df.loc[hosvd_df['selection_score'].idxmax()].to_dict(),
'LDA': lda_df.loc[lda_df['selection_score'].idxmax()].to_dict(),
'RandomForest': rf_df.loc[rf_df['selection_score'].idxmax()].to_dict(),
# 'GradientBoosting': gb_df.loc[gb_df['selection_score'].idxmax()].to_dict(),
'NaiveBayes': nb_df.loc[nb_df['selection_score'].idxmax()].to_dict(),
'SVM': svm_df.loc[svm_df['selection_score'].idxmax()].to_dict(),
'KNN': knn_df.loc[knn_df['selection_score'].idxmax()].to_dict()
}
best_configs = {
'LogisticRegression': r.lr_results.loc[r.lr_results['selection_score'].idxmax()].to_dict(),
'SVD': r.svd_results.loc[r.svd_results['selection_score'].idxmax()].to_dict(),
'HOSVD': hosvd_df.loc[hosvd_df['selection_score'].idxmax()].to_dict(),
'LDA': lda_df.loc[lda_df['selection_score'].idxmax()].to_dict(),
'RandomForest': rf_df.loc[rf_df['selection_score'].idxmax()].to_dict(),
# 'GradientBoosting': gb_df.loc[gb_df['selection_score'].idxmax()].to_dict(),
'NaiveBayes': nb_df.loc[nb_df['selection_score'].idxmax()].to_dict(),
'SVM': svm_df.loc[svm_df['selection_score'].idxmax()].to_dict(),
'KNN': knn_df.loc[knn_df['selection_score'].idxmax()].to_dict()
}
#| warning: false
#| output: false
#| echo: false
#| eval: false
# Find the best configuration for each model family
best_configs = {
'LogisticRegression': r.lr_results.loc[r.lr_results['selection_score'].idxmax()].to_dict(),
'SVD': r.svd_results.loc[r.svd_results['selection_score'].idxmax()].to_dict(),
'HOSVD': hosvd_df.loc[hosvd_df['selection_score'].idxmax()].to_dict(),
'LDA': lda_df.loc[lda_df['selection_score'].idxmax()].to_dict(),
'RandomForest': rf_df.loc[rf_df['selection_score'].idxmax()].to_dict(),
# 'GradientBoosting': gb_df.loc[gb_df['selection_score'].idxmax()].to_dict(),
'NaiveBayes': nb_df.loc[nb_df['selection_score'].idxmax()].to_dict(),
'SVM': svm_df.loc[svm_df['selection_score'].idxmax()].to_dict(),
'KNN': knn_df.loc[knn_df['selection_score'].idxmax()].to_dict()
}
# Dictionary of model training functions
model_functions = {
'LogisticRegression': lambda X_train, X_test, y_train, y_test:
logistic_regression(X_train, X_test, y_train, y_test, best_configs['LogisticRegression']['C']),
'SVD': lambda X_train, X_test, y_train, y_test:
SVD_residual(X_train, X_test, y_train, y_test, best_configs['SVD']['n_components'])
if best_configs['SVD']['classifier'] == 'residual' else
SVD_SVC(X_train, X_test, y_train, y_test, best_configs['SVD']['n_components']),
'HOSVD': lambda X_train, X_test, y_train, y_test:
HOSVD_residual(X_train, X_test, y_train, y_test,
(best_configs['HOSVD']['n_components_mode1'], best_configs['HOSVD']['n_components_mode2']))
if best_configs['HOSVD']['classifier'] == 'residual' else
HOSVD_SVC(X_train, X_test, y_train, y_test,
(best_configs['HOSVD']['n_components_mode1'], best_configs['HOSVD']['n_components_mode2'])),
'LDA': lambda X_train, X_test, y_train, y_test:
lda_classifier(X_train, X_test, y_train, y_test,
best_configs['LDA']['solver'], best_configs['LDA']['shrinkage']),
'RandomForest': lambda X_train, X_test, y_train, y_test:
random_forest(X_train, X_test, y_train, y_test,
best_configs['RandomForest']['n_estimators'],
best_configs['RandomForest']['max_depth'],
best_configs['RandomForest']['min_samples_split']),
# 'GradientBoosting': lambda X_train, X_test, y_train, y_test:
#     gradient_boosting(X_train, X_test, y_train, y_test,
#                     best_configs['GradientBoosting']['n_estimators'],
#                     best_configs['GradientBoosting']['learning_rate'],
#                     int(best_configs['GradientBoosting']['max_depth'])),
'NaiveBayes': lambda X_train, X_test, y_train, y_test:
naive_bayes(X_train, X_test, y_train, y_test, best_configs['NaiveBayes']['var_smoothing']),
'SVM': lambda X_train, X_test, y_train, y_test:
svm_classifier(X_train, X_test, y_train, y_test,
best_configs['SVM']['C'], best_configs['SVM']['kernel'], best_configs['SVM']['gamma']),
'KNN': lambda X_train, X_test, y_train, y_test:
knn_classifier(X_train, X_test, y_train, y_test,
best_configs['KNN']['n_neighbors'], best_configs['KNN']['weights'], best_configs['KNN']['p'])
}
# Create a common train/test split for all evaluations
np.random.seed(48)
g_indices = np.random.permutation(len(X))
split_point = int(len(X) * 0.8)
gtrain_indices = g_indices[:split_point]
gtest_indices = g_indices[split_point:]
X_gtrain, X_gtest = X[gtrain_indices], X[gtest_indices]
y_gtrain, y_gtest = y[gtrain_indices], y[gtest_indices]
# Perform greedy cross-validation
greedy_results = []
remaining_models = list(model_functions.keys())
iteration = 1
while remaining_models:
print(f"\nIteration {iteration} - Evaluating {len(remaining_models)} models")
iteration_results = []
for model_name in remaining_models:
model_func = model_functions[model_name]
start_time = time.time()
y_pred = model_func(X_gtrain, X_gtest, y_gtrain, y_gtest)
elapsed_time = time.time() - start_time
accuracy = accuracy_score(y_gtest, y_pred)
report = classification_report(y_gtest, y_pred, output_dict=True)
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
# Calculate selection score
t_m = 60 * 30  # 30 minutes in seconds
selection_score = (1 / (1 + np.exp(27 - 30 * f1))) - (np.exp(27) / (1 + np.exp(27))) + np.exp(-elapsed_time / t_m)
# Store results for this model
result = {
'iteration': iteration,
'model': model_name,
'mean_accuracy': accuracy,
'mean_precision': precision,
'mean_recall': recall,
'f1_score': f1,
'total_time': elapsed_time,
'selection_score': selection_score
}
# Add the best hyperparameters to the result
best_config = best_configs[model_name]
for param, value in best_config.items():
if param not in ['mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']:
result[param] = value
iteration_results.append(result)
print(f"  {model_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}, Time={elapsed_time:.2f}s, Score={selection_score:.4f}")
# Find the best model in this iteration
iteration_df = pd.DataFrame(iteration_results)
best_idx = iteration_df['selection_score'].idxmax()
best_model = iteration_df.loc[best_idx, 'model']
# Add all results from this iteration to the overall results
greedy_results.extend(iteration_results)
# Keep only the best model for the next iteration
remaining_models.remove(best_model)
print(f"  Best model in iteration {iteration}: {best_model} (Score: {iteration_df.loc[best_idx, 'selection_score']:.4f})")
# Stop after one iteration for greedy approach
break
greedy_df = pd.DataFrame(greedy_results)
greedy_df = greedy_df.sort_values('selection_score', ascending=False).reset_index(drop=True)
def format_model_name(model_data, prefix):
if prefix == 'LogisticRegression':
return f"Logistic Regression (C={model_data['C']})"
elif prefix == 'SVD':
return f"SVD-{model_data['classifier']} (k={model_data['n_components']})"
elif prefix == 'HOSVD':
return f"HOSVD-{model_data['classifier']} (r1={model_data['n_components_mode1']}, r2={model_data['n_components_mode2']})"
elif prefix == 'RandomForest':
return f"Random Forest (t={model_data['n_estimators']}, d={model_data['max_depth'] or 'None'})"
elif prefix == 'GradientBoosting':
return f"Gradient Boosting (t={model_data['n_estimators']}, lr={model_data['learning_rate']}, d={model_data['max_depth']})"
elif prefix == 'NaiveBayes':
return f"Naive Bayes (s={model_data['var_smoothing']})"
elif prefix == 'SVM':
return f"SVM (C={model_data['C']}, k={model_data['kernel']}, g={model_data['gamma']})"
elif prefix == 'KNN':
return f"KNN (k={model_data['n_neighbors']}, w={model_data['weights']}, d={model_data['p']})"
elif prefix == 'LDA':
return f"LDA (solver={model_data['solver']}, shrink={model_data['shrinkage'] or 'None'})"
else:
return prefix
# Format model names in the greedy results dataframe
for i, row in greedy_df.iterrows():
greedy_df.at[i, 'model'] = format_model_name(row, row['model'])
greedy_df = greedy_df[['model', 'mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']].copy()
greedy_df = greedy_df.sort_values('selection_score', ascending=False).reset_index(drop=True)
np.random.seed(48)
g_indices = np.random.permutation(len(X))
split_point = int(len(X) * 0.8)
gtrain_indices = g_indices[:split_point]
gtest_indices = g_indices[split_point:]
X_gtrain, X_gtest = X[gtrain_indices], X[gtest_indices]
y_gtrain, y_gtest = y[gtrain_indices], y[gtest_indices]
# Perform greedy cross-validation
greedy_results = []
remaining_models = list(model_functions.keys())
iteration = 1
while remaining_models:
print(f"\nIteration {iteration} - Evaluating {len(remaining_models)} models")
iteration_results = []
for model_name in remaining_models:
model_func = model_functions[model_name]
start_time = time.time()
y_pred = model_func(X_gtrain, X_gtest, y_gtrain, y_gtest)
elapsed_time = time.time() - start_time
accuracy = accuracy_score(y_gtest, y_pred)
report = classification_report(y_gtest, y_pred, output_dict=True)
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
# Calculate selection score
t_m = 60 * 30  # 30 minutes in seconds
selection_score = (1 / (1 + np.exp(27 - 30 * f1))) - (np.exp(27) / (1 + np.exp(27))) + np.exp(-elapsed_time / t_m)
# Store results for this model
result = {
'iteration': iteration,
'model': model_name,
'mean_accuracy': accuracy,
'mean_precision': precision,
'mean_recall': recall,
'f1_score': f1,
'total_time': elapsed_time,
'selection_score': selection_score
}
# Add the best hyperparameters to the result
best_config = best_configs[model_name]
for param, value in best_config.items():
if param not in ['mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']:
result[param] = value
iteration_results.append(result)
print(f"  {model_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}, Time={elapsed_time:.2f}s, Score={selection_score:.4f}")
# Find the best model in this iteration
iteration_df = pd.DataFrame(iteration_results)
best_idx = iteration_df['selection_score'].idxmax()
best_model = iteration_df.loc[best_idx, 'model']
# Add all results from this iteration to the overall results
greedy_results.extend(iteration_results)
# Keep only the best model for the next iteration
remaining_models.remove(best_model)
print(f"  Best model in iteration {iteration}: {best_model} (Score: {iteration_df.loc[best_idx, 'selection_score']:.4f})")
# Stop after one iteration for greedy approach
break
lr_df = r.lr_results
svd_df = r.svd_results
def SVD_residual(X_train, X_test, y_train, y_test, n_components):
n_components = params['n_components']
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Group training data by digit class
digit_classes = np.unique(y_train)
class_indices = {digit: np.where(y_train == digit)[0] for digit in digit_classes}
# Compute SVD for each digit class separately
class_subspaces = {}
for digit in digit_classes:
# Get data for this digit class
X_digit = X_train_scaled[class_indices[digit]]
# Compute SVD for this class
svd_digit = TruncatedSVD(n_components=n_components)
svd_digit.fit(X_digit)
# Store the SVD components and mean for this digit class
class_subspaces[digit] = {
'components': svd_digit.components_,
'mean': np.mean(X_digit, axis=0)
}
# Function to compute Frobenius norm of residual
def compute_residual(x, subspace):
# Project the sample onto the subspace
components = subspace['components']
mean_centered_x = x - subspace['mean']
projection = mean_centered_x @ components.T @ components
# Compute the residual
residual = mean_centered_x - projection
# Return the Frobenius norm of the residual
return np.linalg.norm(residual)
# Predict test samples using minimum residual
y_pred = []
residuals = np.zeros((X_test_scaled.shape[0], len(digit_classes)))
for i, x in enumerate(X_test_scaled):
# Compute residual for each digit class
for j, digit in enumerate(digit_classes):
residuals[i, j] = compute_residual(x, class_subspaces[digit])
# Predict the digit with minimum residual
y_pred.append(digit_classes[np.argmin(residuals[i])])
# Convert to numpy array
y_pred = np.array(y_pred)
return(y_pred)
def SVD_SVC(X_train, X_test, y_train, y_test, n_components):
# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Apply SVD
svd = TruncatedSVD(n_components=n_components)
X_train_svd = svd.fit_transform(X_train_scaled)
X_test_svd = svd.transform(X_test_scaled)
# Train classifier
clf = SVC()
clf.fit(X_train_svd, y_train)
# Predict
y_pred = clf.predict(X_test_svd)
return(y_pred)
lr_df = r.lr_results
svd_df = r.svd_results
while remaining_models:
print(f"\nIteration {iteration} - Evaluating {len(remaining_models)} models")
iteration_results = []
for model_name in remaining_models:
model_func = model_functions[model_name]
start_time = time.time()
y_pred = model_func(X_gtrain, X_gtest, y_gtrain, y_gtest)
elapsed_time = time.time() - start_time
accuracy = accuracy_score(y_gtest, y_pred)
report = classification_report(y_gtest, y_pred, output_dict=True)
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
# Calculate selection score
t_m = 60 * 30  # 30 minutes in seconds
selection_score = (1 / (1 + np.exp(27 - 30 * f1))) - (np.exp(27) / (1 + np.exp(27))) + np.exp(-elapsed_time / t_m)
# Store results for this model
result = {
'iteration': iteration,
'model': model_name,
'mean_accuracy': accuracy,
'mean_precision': precision,
'mean_recall': recall,
'f1_score': f1,
'total_time': elapsed_time,
'selection_score': selection_score
}
# Add the best hyperparameters to the result
best_config = best_configs[model_name]
for param, value in best_config.items():
if param not in ['mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']:
result[param] = value
iteration_results.append(result)
print(f"  {model_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}, Time={elapsed_time:.2f}s, Score={selection_score:.4f}")
# Find the best model in this iteration
iteration_df = pd.DataFrame(iteration_results)
best_idx = iteration_df['selection_score'].idxmax()
best_model = iteration_df.loc[best_idx, 'model']
# Add all results from this iteration to the overall results
greedy_results.extend(iteration_results)
# Keep only the best model for the next iteration
remaining_models.remove(best_model)
print(f"  Best model in iteration {iteration}: {best_model} (Score: {iteration_df.loc[best_idx, 'selection_score']:.4f})")
# Stop after one iteration for greedy approach
break
greedy_df = pd.DataFrame(best_configs)
def format_model_name(model_data, prefix):
if prefix == 'LogisticRegression':
return f"Logistic Regression (C={model_data['C']})"
elif prefix == 'SVD':
return f"SVD-{model_data['classifier']} (k={model_data['n_components']})"
elif prefix == 'HOSVD':
return f"HOSVD-{model_data['classifier']} (r1={model_data['n_components_mode1']}, r2={model_data['n_components_mode2']})"
elif prefix == 'RandomForest':
return f"Random Forest (t={model_data['n_estimators']}, d={model_data['max_depth'] or 'None'})"
elif prefix == 'GradientBoosting':
return f"Gradient Boosting (t={model_data['n_estimators']}, lr={model_data['learning_rate']}, d={model_data['max_depth']})"
elif prefix == 'NaiveBayes':
return f"Naive Bayes (s={model_data['var_smoothing']})"
elif prefix == 'SVM':
return f"SVM (C={model_data['C']}, k={model_data['kernel']}, g={model_data['gamma']})"
elif prefix == 'KNN':
return f"KNN (k={model_data['n_neighbors']}, w={model_data['weights']}, d={model_data['p']})"
elif prefix == 'LDA':
return f"LDA (solver={model_data['solver']}, shrink={model_data['shrinkage'] or 'None'})"
else:
return prefix
# Format model names in the greedy results dataframe
for i, row in greedy_df.iterrows():
greedy_df.at[i, 'model'] = format_model_name(row, row['model'])
greedy_df = greedy_df[['model', 'mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']].copy()
greedy_df = greedy_df.sort_values('selection_score', ascending=False).reset_index(drop=True)
View(greedy_df)
greedy_df = pd.DataFrame(best_configs).transpose
greedy_df
greedy_df = pd.DataFrame(best_configs).transpose()
View(greedy_df)
greedy_df = pd.DataFrame(best_configs).transpose().index.name = 'model'
greedy_df.reset_index(inplace=True)
greedy_df = pd.DataFrame(best_configs).transpose()
greedy_df.index.name = 'model'
greedy_df.reset_index(inplace=True)
def format_model_name(model_data, prefix):
if prefix == 'LogisticRegression':
return f"Logistic Regression (C={model_data['C']})"
elif prefix == 'SVD':
return f"SVD-{model_data['classifier']} (k={model_data['n_components']})"
elif prefix == 'HOSVD':
return f"HOSVD-{model_data['classifier']} (r1={model_data['n_components_mode1']}, r2={model_data['n_components_mode2']})"
elif prefix == 'RandomForest':
return f"Random Forest (t={model_data['n_estimators']}, d={model_data['max_depth'] or 'None'})"
elif prefix == 'GradientBoosting':
return f"Gradient Boosting (t={model_data['n_estimators']}, lr={model_data['learning_rate']}, d={model_data['max_depth']})"
elif prefix == 'NaiveBayes':
return f"Naive Bayes (s={model_data['var_smoothing']})"
elif prefix == 'SVM':
return f"SVM (C={model_data['C']}, k={model_data['kernel']}, g={model_data['gamma']})"
elif prefix == 'KNN':
return f"KNN (k={model_data['n_neighbors']}, w={model_data['weights']}, d={model_data['p']})"
elif prefix == 'LDA':
return f"LDA (solver={model_data['solver']}, shrink={model_data['shrinkage'] or 'None'})"
else:
return prefix
# Format model names in the greedy results dataframe
for i, row in greedy_df.iterrows():
greedy_df.at[i, 'model'] = format_model_name(row, row['model'])
greedy_df = greedy_df[['model', 'mean_accuracy', 'mean_precision', 'mean_recall', 'f1_score', 'total_time', 'selection_score']].copy()
greedy_df = greedy_df.sort_values('selection_score', ascending=False).reset_index(drop=True)
quit
greedy_results <- py$greedy_df
write_csv(greedy_df, "data/greedy_df.csv")
write_csv(greedy_results, "data/greedy_results.csv")
greedy_results <- read_csv("data/greedy_results.csv")
results_table(
greedy_results,
prefix_cols=list(
model = colDef(name = "Model", minWidth = 140)
)
)
View(greedy_results)
greedy_results <- py$greedy_df
View(greedy_results)
write_csv(greedy_results, "data/greedy_results.csv")
greedy_results <- read_csv("data/greedy_results.csv")
results_table(
greedy_results,
prefix_cols=list(
model = colDef(name = "Model", minWidth = 140)
)
)
View(greedy_results)
greedy_results <- py$greedy_df
write_csv(greedy_results, "data/greedy_results.csv")
greedy_results <- py$greedy_df
View(greedy_results)
View(greedy_results)
greedy_results <- py$greedy_df %>%
mutate(across(c(mean_accuracy, mean_precision, mean_recall, f1_score, total_time, selection_score)))
greedy_results <- py$greedy_df %>%
mutate(across(c(mean_accuracy, mean_precision, mean_recall, f1_score, total_time, selection_score), as.numeric))
View(greedy_results)
write_csv(greedy_results, "data/greedy_results.csv")
greedy_results <- read_csv("data/greedy_results.csv")
results_table(
greedy_results,
prefix_cols=list(
model = colDef(name = "Model", minWidth = 140)
)
)
